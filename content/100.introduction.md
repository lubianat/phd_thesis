# Introduction


<!-- 1.4. The challenges of the Human Cell Atlas -->
## The Human Cell Atlas (HCA) Project

<!-- 1.4.1. The Human Cell Atlas project and its scope -->
<!-- 1.4.1.1. Participants -->
The advent of single-cell technologies has ignited the desire of a deep knowledge on cells, the building blocks of life [@wikidata:Q99418649]. 
The Human Cell Atlas (HCA) project, has been a major player in the cell knowledge ecosystem, running since 2017 towards the task to characterize every cell type in the human body [@wikidata:Q46368626]. The HCA consortium recruited people from all over the world to tackle different parts of the project. In Brazil, Prof. Helder Nakaya (supervisor of this PhD project) is leading the national effort to contribute to HCA, with a focus on the roles of different cell types in the pathological processes of infectious and inflammatory diseases.

<!-- - 1.4.1.2. Overview of main analytical techniques  -->
Building a full atlas of human cells comes with multiple challenges. The project includes detection, in single cells, of RNA content (scRNA-Seq), chromatin accessibility (scATAC-Seq), and protein markers (primarily by CYTOF), as well as spatial information on cells with multiplexed _in situ_ hybridization (such as MERFISH) and imaging mass cytometry [@wikidata:Q46368626 ;@wikidata:Q104450645]. Every lab will contribute with its expertise, providing samples that are representative of human diversity.

HCA is set to revolutionize the biomedical sciences, by creating tools and standards for basic research, as well as allowing better characterization of disease, and thus, ultimately, improving diagnostics and therapy. 
Its products (data, information, knowledge and wisdom) need to be FAIR: findable, accessible, interoperable and reusable.
Data stewardship and data management are growing as core demands of the scientific community, ranging from data management plans [@wikidata:Q56524391] to specialized personnel [@wikidata:Q56524391].

<!-- 1.4.3. Data availability -->
<!-- - 1.4.3.1. As coordinated by the Human Cell Atlas -->
The Human Cell Atlas has a dedicated team for organizing data: the Data Coordination Platform (DCP) [@url:https://data.humancellatlas.org/about] [@wikidata:Q104450645].
The DCP is responsible for tracing the plan for computational interoperability, from the data generators to the consumers.[@wikidata:Q104450645].
The Human Cell Atlas  has its portal for data (<https://data.humancellatlas.org/>) which composes the data repository landscape with other resources, like the Broad Institute Single Cell Portal (<https://singlecell.broadinstitute.org/single_cell>) and the Chan-Zuckerberg Biohub Tabula Sapiens (<https://tabula-sapiens-portal.ds.czbiohub.org/>). 
In addition to its core team, the HCA is poised to grow by community interaction, and states in its opening paper that "As with the Human Genome Project, a robust plan will best emerge from wide-ranging scientific discussions and careful planning".[@wikidata:Q46368626]  
Thus, this project inserts itself among the wide-ranging scientific discussions to improve data - and knowledge - interoperability. 

The highlight of "knowledge" in the last paragraph is meant to stress that data _per se_ is not enough. 
There is a long way from raw datasets to commonly agreed scientific knowledge. 
And, ultimately, this long way is what allows humanity to take advantage of scientific endeavors.
Currently, the gap between data and knowledge is mostly targeted via the writing and sharing of scientific manuscripts, the _de facto_ currency of exchange of claims about the natural world. 
The Human Cell Atlas Publication Commitee reviews and selects publications that are directly part of the HCA.
A set of publications is, thus, one of the major outputs of the whole endeavor.


The challenge that arises, thus, is one of managing a wealth of information and cast it into useful science.
Experimental articles that analyze thousands of cells pose an overload of information alone. 
Ideally, we would like to understand, remember and make use of every statement produced by the HCA.
As this goal is humanely impossible, we need to develop tools to make the knowledge interoperable with the aid of computers. 
At that point, the challenges of the HCA enter in resonance with the challenges of text-mining, biocuration and literature based discovery, which will be discussed in the chapter <!-- - 1.1. The quest for interoperable knowledge --> of this introduction. 
## Literature Based Discovery, hidden knowledge and text-mining
 <!-- - 1.1. The quest for interoperable knowledge -->
It is not recent news that the amount of scholar information vastly outnumbers what single researchers can fathom. 
Nevertheless, the gap between single individuals and the collectively body of knowledge has been widening in 
an accelerated fashion.
The explosion in the number of published articles is leading to a "tsunami of knowlegde",
flooding the scientific literature with rich information. 
Moreover, articles themselves are becoming denser, as high-throughput (and high-information) technologies
like single-cell RNA-sequencing get cheaper and widely used. 

 <!-- - 1.1. The quest for interoperable knowledge -->
 <!-- - 1.2.3. Web of Data and Linked Open Data -->
The technological advances, however, are no yet met by equivalent knowledge-handling systems.
Mainstream scientific publication is, nowadays, barely readable by machines.
Articles are written for human consumption, using ambiguous natural language and relying on implicit conventions. 
Tables and data rarely make use of URI (Uniform Resource Identifiers), RDF (Resource Description Framework) formating and other W3C (World Wide Web Standards). 
In fact, those standards and their acronyms are completely foreign for most life scientists (personal observations), despite
being the _de jure_ gold standard for data quality. [@url:https://5stardata.info/en/]
Thus, interconnecting biomedical knowledge is an open challenge of our century, and there is a large way to go
before society can fully benefit from the sum of all knowledge we generate. 

<!--- 1.1.1. Literature Based Discovery, hidden knowledge and text-mining -->
The scientific community has pursue solutions for this tsunami of information from many different angles. 
Narrative reviews, systematic reviews and textbooks compile and synthetize information, providing a layer of processing. 
Biocuration efforts go a step further and transform unstructured information into structered information
in knowledgebases, such as UniProt and PDB. 
Text-mining apply a range of Natural Language Processing tools to try and extract biological relations,
or provide guidance for biocurators. 
Elaborate knowlegde networks, like the STRING database [@wikidata:Q102383784] and Wikidata[@wikidata:Q87830400], 
combine information from different sources. 
Overall, approaches mix and intermingle to mature hidden knowledge into solid theories and practical applications. 

<!-- - 1.1.1.1. Literature Based Discovery (explicitly) --->

The synthesis effort of literature mining goes beyond purely detecting what science claims to be true. 
Interconnected knowledge provides a way to discover new, implicit knowledge, by applying logical reasoning to a dataset. 
A field denominated Literature Based Discovery [@url:https://en.wikipedia.org/wiki/Literature-based_discovery] dedicates itself to this challenge: make actual discoveries (or at least very strong hypothesis) using as material plainly the existing literature. [@wikidata:Q38371706]
The textbook example of Literature Based Discovery is described by Don Swanson's so-called ABC model: If A is related do B, and B is related to C, then A and C are indirectly related [@wikidata:Q36280460]
In a seminal paper, Swanson showed an hypothesis about using fish oil (A) to treat Raynauld's disease (C), demonstrating that even though the specialized fish-oil (A) literature had shown its association (AB) with a set of blood parameters (B), and the specialized Raynauld's disease literature had show its association (BC) with the same set of parameters (B), the AC link was never made in the literature, despite its seeming obviousness [@wikidata:Q36280460]

<!--- 1.1.1. Literature Based Discovery, hidden knowledge and text-mining -->

Modern advancements of literature-based discovery rely on Natural Language Processing, Machine Learning and Knowledge graphs to make inferences on literature knowledge.
Word embeddings, for example, are leading inference of properties of compounds based on their shared neighbourhood of words (the words before and after their mentiongs) with known compounds, thus making use of latent knowledge in the body of knowledge. [@wikidata:Q91595456]
Other, more explicit approaches, rely on extracted relations embedded in knowledge graphs,fo example, the discovery of new RNA-binding proteins related to Amyotrophic Lateral Sclerosis by analysis of the Watson Drug Discovery gene-disease network. [@wikidata:Q47406275]

Knowledge graphs have a set of characteristics that make then useful for Literature Based Discovery: the power of representing multiple relations, the power of making inferences on top of those relations, and provide human understandability at every step, allowing for a dialog between expert humans and computing systems.
The field of biomedical ontologies explores that direction in depth, and the community is building many solutions, widely applicable for the biomedical sciences.


For the Human Cell Atlas Project (as presented in the chapter <!-- 1.4. The challenges of the Human Cell Atlas -->) to maximize its benefit for society,  its knowledge products will need to be inserted into the main route of automated knowledge discovery . 
That implies a daunting task of building knowledge graphs able to deal with it at all layers, including the generated data and metadata, its range of different protocols, and the purified knowledge projects that are enshrined in publications.
Thus, the chapter <!-- - 1.2. Formal representation of knowledge and - 1.3. Knowledge Representation in biology -->  will present challenges and paths for applying literature based discovery on an enormous scale and with sufficient flexibility to deal with the Human Cell Atlas. 

## Knowledge graphs as tools for interoperability
<!-- - 1.2. Formal representation of knowledge and - 1.3. Knowledge Representation in biology --> 
* The OBO Foundry and biomedical ontologies

* Knowledge graphs and a different approach to biomedical semantics

* Wikidata as a knowledge graph for the life sciences